@article{AdamSGD,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T14:24:27.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  temp_url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
  volume = {abs/1412.6980},
  year = 2014
}

@ARTICLE{Chawla02smote:synthetic,
    author = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
    title = {SMOTE: Synthetic Minority Over-sampling Technique},
    journal = {Journal of Artificial Intelligence Research},
    year = {2002},
    volume = {16},
    pages = {321--357}
}

@article{Zhongwe,
  title={Watershed superpixel},
  author={Zhongwen Hu and Qin Zou and Qingquan Li},
  journal={2015 IEEE International Conference on Image Processing (ICIP)},
  year={2015},
  pages={349-353},
  keywords={computer vision;image resolution;image segmentation;watershed superpixel algorithm;computer-vision application;real-time vision system;high-efficient superpixel algorithm;spatial-constrained watershed;SCoW;marker-controlled manner;superpixel boundary;image edge;Image edge detection;Image segmentation;Shape;Clustering algorithms;Transforms;Visualization;Real-time systems;superpixel;watershed;spatial constraint;image segmentation},
doi={10.1109/ICIP.2015.7350818}
}

@article{Achanta10slicsuperpixels,
  title={SLIC superpixels compared to state-of-the-art superpixel methods},
  author={Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={11},
  pages={2274--2282},
  year={2012},
  publisher={Institute of Electrical and Electronics Engineers, Inc., 345 E. 47 th St. NY NY 10017-2394 United States}
}

@ARTICLE{Wernick2010,
author={M. N. Wernick and Y. Yang and J. G. Brankov and G. Yourganov and S. C. Strother},
journal={IEEE Signal Processing Magazine},
title={Machine Learning in Medical Imaging},
year={2010},
volume={27},
number={4},
pages={25-38},
keywords={learning (artificial intelligence);medical image processing;machine learning;medical imaging;computer-aided diagnosis;CAD;functional brain mapping;Machine learning;Biomedical imaging;Predictive models;Support vector machines;Brain mapping;Supervised learning;Cancer;Computer aided diagnosis;Image retrieval;Content based retrieval},
doi={10.1109/MSP.2010.936730},
ISSN={1053-5888},
month={July}}

@article{Rajat2018,
  author    = {Rajat Kumar Sinha and
               Ruchi Pandey and
               Rohan Pattnaik},
  title     = {Deep Learning For Computer Vision Tasks: {A} review},
  journal   = {CoRR},
  volume    = {abs/1804.03928},
  year      = {2018},
  temp_url       = {http://arxiv.org/abs/1804.03928},
  archivePrefix = {arXiv},
  eprint    = {1804.03928},
  timestamp = {Mon, 13 Aug 2018 16:47:36 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Srivastava:2014:DSW:2627435.2670313,
 author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
 title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2014},
 volume = {15},
 number = {1},
 month = jan,
 year = {2014},
 issn = {1532-4435},
 pages = {1929--1958},
 numpages = {30},
 temp_url = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
 acmid = {2670313},
 publisher = {JMLR.org},
 keywords = {deep learning, model combination, neural networks, regularization},
}

@ARTICLE{Houlsby2011,
   author = {Houlsby, Neil and Husz{\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
   title = {Bayesian Active Learning for Classification and Preference Learning},
   journal = {ArXiv e-prints},
   archivePrefix = "arXiv",
   eprint = {1112.5745},
   primaryClass = "stat.ML",
   keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
   year = 2011,
   temp_url = {https://arxiv.org/abs/1112.5745},
   temp_url = {http://adsabs.harvard.edu/abs/2011arXiv1112.5745H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Ertekin:2007:LBA:1321440.1321461,
  abstract = {This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced data-sets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire data-set. The proposed method yields an efficient querying system and allows active learning to be applied to very large data-sets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.},
  acmid = {1321461},
  added-at = {2012-02-13T12:49:12.000+0100},
  address = {New York, NY, USA},
  author = {Ertekin, Seyda and Huang, Jian and Bottou, Leon and Giles, Lee},
  booktitle = {Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
  description = {Learning on the border},
  doi = {10.1145/1321440.1321461},
  interhash = {165bf30419d44546b19fd2c3ee6af6a5},
  intrahash = {a2d83cc1c8ea934bc8868f2356706979},
  isbn = {978-1-59593-803-9},
  keywords = {readthis},
  location = {Lisbon, Portugal},
  numpages = {10},
  pages = {127--136},
  publisher = {ACM},
  series = {CIKM '07},
  timestamp = {2012-02-13T12:49:12.000+0100},
  title = {Learning on the border: active learning in imbalanced data classification},
  temp_url = {http://doi.acm.org/10.1145/1321440.1321461},
  year = 2007
}

@inproceedings{BMVC.25.78,
title = {Combining Self Training and Active Learning for Video Segmentation},
author = {Alireza Fathi and  Maria Florina Balcan and  Xiaofeng Ren and  James M. Rehg},
year={2011},
pages={78.1--78.11},
booktitle={Proceedings of the British Machine Vision Conference},
publisher={BMVA Press},
editors={Hoey, Jesse and McKenna, Stephen and Trucco, Emanuele},
isbn={1-901725-43-X},
temp_url = {http://dx.doi.org/10.5244/C.25.78}
}

@InProceedings{Yarin2017b,
  title = 	 {Deep {B}ayesian Active Learning with Image Data},
  author = 	 {Yarin Gal and Riashat Islam and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1183--1192},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  temp_pdf = 	 {http://proceedings.mlr.press/v70/gal17a/gal17a.pdf},
  temp_url = 	 {http://proceedings.mlr.press/v70/gal17a.html},
  abstract = 	 {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST data-set, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).}
}

@inproceedings{Krizhevsky2012,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {1097--1105},
 numpages = {9},
 temp_url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
 acmid = {2999257},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@PhdThesis{Yarin2017,
  title={Uncertainty in Deep Learning},
  author={Gal, Yarin},
  year={2016},
  school={University of Cambridge}
}

@article{Shelhamer,
 author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
 title = {Fully Convolutional Networks for Semantic Segmentation},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 issue_date = {April 2017},
 volume = {39},
 number = {4},
 month = apr,
 year = {2014},
 issn = {0162-8828},
 pages = {640--651},
 numpages = {12},
 temp_url = {https://doi.org/10.1109/TPAMI.2016.2572683},
 doi = {10.1109/TPAMI.2016.2572683},
 acmid = {3069246},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Kaiming2018,
  author    = {Kaiming He and
               Georgia Gkioxari and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Mask {R-CNN}},
  journal   = {CoRR},
  volume    = {abs/1703.06870},
  year      = {2017},
  temp_url       = {http://arxiv.org/abs/1703.06870},
  archivePrefix = {arXiv},
  eprint    = {1703.06870},
  timestamp = {Mon, 13 Aug 2018 16:46:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeGDG17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Shaoqing2015,
title = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {91--99},
year = {2015},
publisher = {Curran Associates, Inc.},
temp_url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}

@InProceedings{Ross2015,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@InProceedings{Ross2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@incollection{Ciresan2012,
title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
author = {Dan Ciresan and Alessandro Giusti and Luca M. Gambardella and Schmidhuber, J\"{u}rgen},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2843--2851},
year = {2012},
publisher = {Curran Associates, Inc.},
temp_url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf}
}

@article{Olaf2015,
  author    = {Olaf Ronneberger and
               Philipp Fischer and
               Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1505.04597},
  year      = {2015},
  temp_url       = {http://arxiv.org/abs/1505.04597},
  archivePrefix = {arXiv},
  eprint    = {1505.04597},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RonnebergerFB15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VonLuxburg2007,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {0711.0189},
author = {{Von Luxburg}, Ulrike},
doi = {10.1007/s11222-007-9033-z},
eprint = {0711.0189},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/tutorial{\_}on{\_}spectral{\_}clustering:},
isbn = {0960-3174},
issn = {09603174},
journal = {Stat. Comput.},
keywords = {Graph Laplacian,Spectral clustering},
number = {4},
pages = {395--416},
pmid = {19784854},
title = {{A tutorial on spectral clustering}},
volume = {17},
year = {2007}
}
@article{Yu2003,
abstract = {We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported.},
author = {Yu and Shi},
doi = {10.1109/ICCV.2003.1238361},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/multiclass{\_}spectral{\_}clustering.pdf:pdf},
isbn = {0-7695-1950-4},
journal = {Proc. Ninth IEEE Int. Conf. Comput. Vis.},
number = {1},
pages = {313--319 vol.1},
title = {{Multiclass spectral clustering}},
temp_url = {http://ieeexplore.ieee.org/document/1238361/},
year = {2003}
}

@article{Shi2000,
abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph},
archivePrefix = {arXiv},
arxivId = {cs/0703101v1},
author = {Shi, Jianbo and Malik, Jitendra},
doi = {10.1109/34.868688},
eprint = {0703101v1},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/Normalized{\_}Cuts{\_}and{\_}Image{\_}Segmentation.pdf:pdf},
isbn = {0-8186-7822-4},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {8},
pages = {888--905},
pmid = {15742889},
primaryClass = {cs},
title = {{Normalized cuts and image segmentation}},
volume = {22},
year = {2000}
}
@article{Schlesinger2011,
author = {Schlesinger, Mi and Hlav{\'{a}}c, V},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schlesinger, Hlav{\'{a}}c - 2011 - Supervised and unsupervised learning.pdf:pdf},
isbn = {9781139042918},
journal = {Artif. Intell.},
number = {April},
title = {{Supervised and unsupervised learning.}},
temp_url = {https://cw.felk.cvut.cz/lib/exe/fetch.php/courses/a3m33ui/prednasky/learning{\_}sup{\_}unsup-slides.pdf},
year = {2011}
}
@article{Wang2014,
abstract = {We present a simple noise-robust margin-based active learning algorithm to find homogeneous (passing the origin) linear separators and analyze its error convergence when labels are corrupted by noise. We show that when the imposed noise satisfies the Tsybakov low noise condition (Mammen, Tsybakov, and others 1999; Tsybakov 2004) the algorithm is able to adapt to unknown level of noise and achieves optimal statistical rate up to poly-logarithmic factors. We also derive lower bounds for margin based active learning algorithms under Tsybakov noise conditions (TNC) for the membership query synthesis scenario (Angluin 1988). Our result implies lower bounds for the stream based selective sampling scenario (Cohn 1990) under TNC for some fairly simple data distributions. Quite surprisingly, we show that the sample complexity cannot be improved even if the underlying data distribution is as simple as the uniform distribution on the unit ball. Our proof involves the construction of a well separated hypothesis set on the d-dimensional unit ball along with carefully designed label distributions for the Tsybakov noise condition. Our analysis might provide insights for other forms of lower bounds as well.},
archivePrefix = {arXiv},
arxivId = {1406.5383},
author = {Wang, Yining and Singh, Aarti},
eprint = {1406.5383},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Singh - 2014 - Noise-adaptive Margin-based Active Learning and Lower Bounds under Tsybakov Noise Condition.pdf:pdf},
isbn = {9781577357605},
keywords = {Technical Papers: Machine Learning Methods},
pages = {2180--2186},
title = {Noise-adaptive Margin-based Active Learning and Lower Bounds under Tsybakov Noise Condition},
temp_url = {http://arxiv.org/abs/1406.5383},
year = {2014}
}

@article{Hsu2010,
abstract = {This dissertation develops and analyzes active learning algorithms for binary classification problems. In passive (non-active) learning, a learner uses a random sample of labeled examples from a fixed distribution to select a hypothesis with low error. In active learning, a learner receives only a sample of unlabeled data, but has the option to query the label of any of these data points. The hope is that the active learner needs to query the labels of just a few, carefully chosen points in order to produce a hypothesis with low error.$\backslash$nThe first part of this dissertation develops algorithms based on maintaining a version spaceâ€”the set of hypotheses still in contention to be selected. The version space is specifically designed to tolerate arbitrary label noise and model mismatch in the agnostic learning model. The algorithms maintain the version space using a reduction to a special form of agnostic learning that allows for example-based constraints; this represents a computational improvement over previous methods. The generalization behavior of one of these algorithms is rigorously analyzed using a quantity called the disagreement coefficient. This algorithm is shown to have label complexity that improves over that of previous methods, and matches known label complexity lower bounds in certain cases.$\backslash$nThe second part of this dissertation develops algorithms based on simpler reductions to agnostic learning that more closely match the standard abstraction of supervised learning procedures. The generalization behavior of these algorithms are also analyzed in the agnostic learning model, and are shown to have label com- plexity similar to the version space methods. Therefore, these algorithms represent qualitative improvements over version space methods, as strict version space meth- ods can be risky to deploy in practice. The first of these algorithms is based on a relaxation of a version space method, and the second is based on an importance weighting technique. The second algorithm is also shown to automatically adapt to various noise conditions that imply a tighter label complexity analysis. Exper- iments using this algorithm are also presented to illustrate some of the promise of the method.},
archivePrefix = {arXiv},
arxivId = {1708.00088},
author = {Hsu, Daniel},
doi = {10.1300/J122v22n03_06},
eprint = {1708.00088},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu - 2010 - Algorithms for active learning.pdf:pdf},
isbn = {0-8247-0973-X},
issn = {0194-262X},
journal = {Act. Learn.},
title = {{Algorithms for active learning}},
temp_url = {https://escholarship.org/uc/item/3hs1z568.pdf},
year = {2010}
}

@inproceedings{Settles2008,
title = "Multiple-instance active learning",
abstract = "We present a framework for active learning in the multiple-instance (MI) setting. In an MI learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We consider the particular case in which an MI learner is allowed to selectively query unlabeled instances from positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image retrieval and text classification.",
author = "Burr Settles and Mark Craven and Soumya Ray",
language = "English (US)",
isbn = "160560352X",
booktitle = "Advances in Neural Information Processing Systems 20 - Proceedings of the 2007 Conference",
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Settles, Craven, Ray - 2008 - Multiple - Instance Active Learning.pdf:pdf},
pages = {1289--1296},
volume = {20},
year = {2008},
}

@article{Seeger2008,
abstract = {The linear model with sparsity-favouring prior on the coefficients has$\backslash$nimportant applications in many different domains. In machine learning,$\backslash$nmost methods to date search for maximum a posteriori sparse solutions and$\backslash$nneglect to represent posterior uncertainties. In this paper, we address$\backslash$nproblems of Bayesian optimal design (or experiment planning), for which$\backslash$naccurate estimates of uncertainty are essential. To this end, we employ$\backslash$nexpectation propagation approximate inference for the linear model with$\backslash$nLaplace prior, giving new insight into numerical stability properties and$\backslash$nproposing a robust algorithm. We also show how to estimate model$\backslash$nhyperparameters by empirical Bayesian maximisation of the marginal likelihood,$\backslash$nand propose ideas in order to scale up the method to very large$\backslash$nunderdetermined problems.$\backslash$n$\backslash$n$\backslash$nWe demonstrate the versatility of our framework on the application of$\backslash$ngene regulatory network identification from micro-array expression data,$\backslash$nwhere both the Laplace prior and the active experimental design approach are$\backslash$nshown to result in significant improvements. We also address the problem of$\backslash$nsparse coding of natural images, and show how our framework can be used$\backslash$nfor compressive sensing tasks.$\backslash$n$\backslash$n$\backslash$nPart of this work appeared in Seeger et al. (2007b). The gene network$\backslash$nidentification application appears in Steinke et al. (2007).},
author = {Seeger, Matthias W},
file = {:home/charley/Desktop/Prof. lehel's suggestions/seeger08a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {Bayesian methods,optimal design,sparsity},
pages = {759--813},
title = {{Bayesian inference and optimal design for the sparse linear model}},
temp_url = {http://jmlr.csail.mit.edu/papers/v9/seeger08a.html},
volume = {9},
year = {2008}
}

@article{Teytaud2007,
abstract = {We study active learning as a derandomized form of sampling. We show that full derandomization is not suitable in a robust framework, propose partially derandomized samplings, and develop new active learning methods (i) in which expert knowledge is easy to integrate (ii) with a parameter for the exploration/exploitation dilemma (iii) less randomized than the full-random sampling (yet also not deterministic). Experiments are performed in the case of regression for value-function learning on a continuous domain. Our main results are (i) efficient partially derandomized point sets (ii) moderate-derandomization theorems (iii) experimental evidence of the importance of the frontier (iv) a new regression-specific user-friendly sampling tool lessrobust than blind samplers but that sometimes works very efficiently in large dimensions. All experiments can be reproduced by downloading the source code and running the provided command line.},
author = {Teytaud, Olivier and Gelly, Sylvain and Mary, J},
file = {:home/charley/Desktop/Prof. lehel's suggestions/ldsfordp.pdf:pdf},
journal = {Mach. Learn.},
title = {{Active learning in regression, with an application to stochastic dynamic programming}},
temp_url = {http://hal.inria.fr/inria-00173204/},
year = {2007}
}

@article{Cohn1996,
abstract = {For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.},
archivePrefix = {arXiv},
arxivId = {cs/9603104},
author = {Cohn, David A. and Ghahramani, Zoubin and Jordan, Michael I.},
doi = {10.1613/jair.295},
eprint = {9603104},
file = {:home/charley/Desktop/Prof. lehel's suggestions/live-295-1554-jair.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {J. Artif. Intell. Res.},
pages = {129--145},
primaryClass = {cs},
title = {Active learning with statistical models},
volume = {4},
year = {1996}
}


@article{Lee2018,
author = {Lee, C and Chung, A and Wu, E and Chen, A and Lin, T},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/libact.pdf:pdf},
title = {{libact Documentation}},
year = {2018}
}
@article{Wei2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.08865v2},
author = {Wei, Kai and Iyer, Rishabh and Wang, Shengjie and Bilmes, Jeff},
eprint = {arXiv:1510.08865v2},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/mixed{\_}robust{\_}average submodular partition$\backslash$: fast alg, guarantees and application to parallel machine learning and multi-label image segmentation.pdf:pdf},
number = {Ml},
pages = {1--46},
title = {{Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications to Parallel Machine Learning and Multi-Label Image Segmentation}},
year = {2009}
}

@article{Konyushkova2015,
abstract = {We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.04955v1},
author = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
doi = {10.1109/ICCV.2015.340},
eprint = {arXiv:1508.04955v1},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/introducing geometry in active learning for image segmentation.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {2974--2982},
title = {{Introducing geometry in active learning for image segmentation}},
volume = {2015 Inter},
year = {2015}
}

@article{Hoi,
author = {Hoi, Steven C H and Lyu, Michael R},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/semi-supervised SVM batch mode active learning for image retrieval.pdf:pdf},
isbn = {9781424422432},
title = {{Semi-Supervised SVM Batch Mode Active Learning for Image Retrieval}}
}

@article{Kr2013,
author = {Kr, Philipp},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/segmentation.pdf:pdf},
title = {{Image Segmentation Goal : identify groups of pixels that go together}},
year = {2013}
}

@article{Planck2007,
author = {Planck, Max and Luxburg, Ulrike Von and Luxburg, Ulrike Von},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/spectral clustering{\_}luxburg06{\_}TR{\_}v2{\_}4139{\_}tutorial.pdf:pdf},
number = {March},
title = {{A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering}},
year = {2007}
}

@article{Plumat2011,
author = {Plumat, Jerome},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/solving{\_}image{\_}problem{\_}with{\_}graph{\_}cuts.pdf:pdf},
pages = {1--22},
title = {{Markov Random Field : definitions}},
year = {2011}
}
@article{,
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/undirected graphical models.pdf:pdf},
title = {19 19.1},
volume = {2},
year = {1965}
}

@article{Gal2016,
author = {Gal, Yarin},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/uncertainty in deep learning.pdf:pdf},
number = {September},
title = {{Uncertainty in Deep Learning}},
year = {2016}
}

@article{Krahenbuhl2012,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Kr{\"{a}}henb{\"{u}}hl, Philipp and Koltun, Vladlen},
eprint = {1210.5644},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/efficient{\_}inference{\_}in{\_}fully{\_}connected{\_}crfs{\_}with{\_}gaussian{\_}edge{\_}potentials.pdf:pdf},
isbn = {9781618395993},
issn = {9781618395993},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
temp_url = {http://arxiv.org/abs/1210.5644},
year = {2012}
}

@article{Kim2011,
abstract = {The saliency of regions or objects in an image can be significantly boosted if they recur in multiple images. Leveraging this idea, cosegmentation jointly segments common regions from multiple images. In this paper, we propose CoSand, a distributed cosegmentation approach for a highly variable large-scale image collection. The segmentation task is modeled by temperature maximization on anisotropic heat diffusion, of which the temperature maximization with finite K heat sources corresponds to a K-way segmentation that maximizes the segmentation confidence of every pixel in an image. We show that our method takes advantage of a strong theoretic property in that the temperature under linear anisotropic diffusion is a submodular function; therefore, a greedy algorithm guarantees at least a constant factor approximation to the optimal solution for temperature maximization. Our theoretic result is successfully applied to scalable cosegmentation as well as diversity ranking and single-image segmentation. We evaluate CoSand on MSRC and ImageNet data-sets, and show its competence both in competitive performance over previous work, and in much superior scalability.},
author = {Kim, Gunhee and Xing, Eric P. and Fei-Fei, Li and Kanade, Takeo},
doi = {10.1109/ICCV.2011.6126239},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/distributed cosegmentation via submodular optimization.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {169--176},
title = {{Distributed cosegmentation via submodular optimization on anisotropic diffusion}},
year = {2011}
}

@article{Boykov2004,
abstract = {Minimum cut/maximum flow algorithms on graphs have emerged as an increasingly useful tool for exactor approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-Tarjan style "push -relabel" methods and algorithms based on Ford-Fulkerson style "augmenting paths." We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and segmentation. In many cases, our new algorithm works several times faster than any of the other methods, making near real-time performance possible. An implementation of our max-flow/min-cut algorithm is available upon request for research purposes.},
archivePrefix = {arXiv},
arxivId = {cs/0703101v1},
author = {Boykov, Y. and Kolmogorov, V.},
doi = {10.1109/TPAMI.2004.60},
eprint = {0703101v1},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/experimental{\_}comparison{\_}of{\_}min{\_}cut{\_}max{\_}flow{\_}algorithms{\_}for{\_}energy{\_}minimization.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {9},
pages = {1124--1137},
pmid = {15742889},
primaryClass = {cs},
title = {{An experimental comparison of min-cut/max- flow algorithms for energy minimization in vision}},
temp_url = {http://ieeexplore.ieee.org/document/1316848/},
volume = {26},
year = {2004}
}

@article{Litjens2014,
title = "Evaluation of prostate segmentation algorithms for {MRI}: The {PROMISE12} challenge",
abstract = "Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p < 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/.",
keywords = "Challenge, MRI, Prostate, Segmentation",
author = "Geert Litjens and Robert Toth and {van de Ven}, Wendy and Caroline Hoeks and Sjoerd Kerkstra and {van Ginneken}, Bram and Graham Vincent and Gwenael Guillard and Neil Birbeck and Jindang Zhang and Robin Strand and Filip Malmberg and Yangming Ou and Christos Davatzikos and Matthias Kirschner and Florian Jung and Jing Yuan and Wu Qiu and Qinquan Gao and Edwards, {Philip Eddie} and Bianca Maan and {van der Heijden}, Ferdinand and Soumya Ghose and Jhimli Mitra and Jason Dowling and Dean Barratt and Henkjan Huisman and Anant Madabhushi",
year = "2014",
month = "2",
temp_doi = "10.1016/j.media.2013.12.002",
language = "English (US)",
volume = "18",
pages = "359--373",
journal = "Medical Image Analysis",
issn = "1361-8415",
publisher = "Elsevier",
number = "2"
}

@article{Zhang2014,
abstract = {Weakly-supervised image segmentation is a challenging problem with multidisciplinary applications in multimedia content analysis and beyond. It aims to segment an image by leveraging its image-level semantics (i.e., tags). This paper presents a weakly-supervised image segmentation algorithm that learns the distribution of spatially structural superpixel sets from image-level labels. More specifically, we first extract graphlets from a given image, which are small-sized graphs consisting of superpixels and encapsulating their spatial structure. Then, an efficient manifold embedding algorithm is proposed to transfer labels from training images into graphlets. It is further observed that there are numerous redundant graphlets that are not discriminative to semantic categories, which are abandoned by a graphlet selection scheme as they make no contribution to the subsequent segmentation. Thereafter, we use a Gaussian mixture model (GMM) to learn the distribution of the selected post-embedding graphlets (i.e., vectors output from the graphlet embedding). Finally, we propose an image segmentation algorithm, termed representative graphlet cut, which leverages the learned GMM prior to measure the structure homogeneity of a test image. Experimental results show that the proposed approach outperforms state-of-the-art weakly-supervised image segmentation methods, on five popular segmentation data sets. Besides, our approach performs competitively to the fully-supervised segmentation models.},
author = {Zhang, Luming and Gao, Yue and Xia, Yingjie and Lu, Ke and Shen, Jialie and Ji, Rongrong},
doi = {10.1109/TMM.2013.2293424},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/f8d32f6a5bb63c2e86713bc3202a2dd5b3b8.pdf:pdf},
isbn = {1520-9210 VO - 16},
issn = {15209210},
journal = {IEEE Trans. Multimed.},
keywords = {Structure cues,active learning,graphlet,segmentation,weakly supervised},
number = {2},
pages = {470--479},
title = {{Representative discovery of structure cues for weakly-supervised image segmentation}},
volume = {16},
year = {2014}
}

@article{Gal2017,
abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST data-set, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
eprint = {1703.02910},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/deep bayesian active learning.pdf:pdf},
issn = {1938-7228},
title = {{Deep Bayesian Active Learning with Image Data}},
temp_url = {http://arxiv.org/abs/1703.02910},
year = {2017}
}

@article{Rother2004,
abstract = {The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for "border matting" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rother, Carsten and Kolmogorov, Vladimir and Blake, Andrew},
doi = {10.1145/1015706.1015720},
eprint = {arXiv:1011.1669v3},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/grabcut-siggraph04.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Trans. Graph.},
keywords = {alpha matting,foreground extraction,graph cuts,image editing,interactive image segmentation},
number = {3},
pages = {309},
pmid = {3576836516423255393},
title = {{"GrabCut": interactive foreground extraction using iterated graph cuts}},
temp_url = {http://discovery.ucl.ac.uk/153675/{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1015706.1015720},
volume = {23},
year = {2004}
}

@article{Koller2007,
abstract = {Probabilistic graphical models are an elegant framework which combines uncertainty (probabilities) and logical structure (independence constraints) to compactly represent complex, real-world phenomena. The framework is quite general in that many of the commonly proposed statistical models (Kalman filters, hidden Markov models, Ising models) can be described as graphical models. Graphical models have enjoyed a surge of interest in the last two decades, due both to the flexibility and power of the representation and to the increased ability to effectively learn and perform inference in large networks.},
author = {Koller, Daphne and Friedman, Nir and Getoor, Lise and Taskar, Ben},
doi = {10.1.1.146.2935},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/graphical model in nutshell.pdf:pdf},
isbn = {0262072882},
journal = {Introd. to Stat. Relational Learn.},
pages = {43},
title = {{Graphical Models in a Nutshell}},
temp_url = {http://www.robotics.stanford.edu/{~}koller/Papers/Koller+al:SRL07.pdf},
year = {2007}
}

@article{Vezhnevets2012,
abstract = {We address the problem of semantic segmentation: classifying each pixel in an image according to the semantic class it belongs to (e.g. dog, road, car). Most existing methods train from fully supervised images, where each pixel is annotated by a class label. To reduce the annotation effort, recently a few weakly supervised approaches emerged. These require only image labels indicating which classes are present. Although their performance reaches a satisfactory level, there is still a substantial gap between the accuracy of fully and weakly supervised methods. We address this gap with a novel active learning method specifically suited for this setting. We model the problem as a pairwise CRF and cast active learning as finding its most informative nodes. These nodes induce the largest expected change in the overall CRF state, after revealing their true label. Our criterion is equivalent to maximizing an upper-bound on accuracy gain. Experiments on two data-sets show that our method achieves 97{\%} percent of the accuracy of the corresponding fully supervised model, while querying less than 17{\%} of the (super-)pixel labels.},
author = {Vezhnevets, Alexander and Buhmann, Joachim M. and Ferrari, Vittorio},
temp_doi = {10.1109/CVPR.2012.6248050},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/active learning for semantic segmentation with expected change.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3162--3169},
temp_url = {doi.ieeecomputersociety.org/10.1109/CVPR.2012.6247757},
title = {{Active learning for semantic segmentation with expected change}},
year = {2012}
}

@article{Lafferty2001,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.4088v1},
author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando C N},
temp_doi = {10.1038/nprot.2006.61},
eprint = {arXiv:1011.4088v1},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/conditional random fields$\backslash$: probabilistic models for segmenting and labeling sequence data.pdf:pdf},
isbn = {1558607781},
issn = {1750-2799},
journal = {ICML '01 Proc. Eighteenth Int. Conf. Mach. Learn.},
keywords = {Animals,Antigen,Gene Transfer Techniques,Genetic Engineering,Genetic Engineering: methods,Genetic Vectors,Genetic Vectors: metabolism,Mice,Receptors,Retroviridae,Retroviridae: genetics,Stem Cells,Stem Cells: metabolism,T-Cell,Transgenic,Transgenic: genetics,Transgenic: metabolism,alpha-beta,alpha-beta: genetics,alpha-beta: metabolism},
number = {June},
pages = {282--289},
pmid = {17406263},
title = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
temp_url = {http://repository.upenn.edu/cis{\_}papers/159/{\%}5Cnhttp://dl.acm.org/citation.cfm?id=655813},
volume = {8},
year = {2001}
}
@article{Zheng2015,
abstract = {Pixel-level labelling tasks, such as semantic segmentation and depth estimation from single RGB image, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. To solve this problem, we introduce a new form of convolutional neural network, called CRF-RNN, which expresses a Conditional Random Field (CRF) as a Recurrent Neural Network (RNN). Our short network can be plugged in as a part of a deep Convolutional Neural Network (CNN) to obtain an end-to-end system that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole system end-to-end with the usual back-propagation algorithm. We apply this framework to the problem of semantic image segmentation, obtaining competitive results with the state-of-the-art without the need of introducing any post-processing method for object delineation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03240v1},
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip},
doi = {10.1109/ICCV.2015.179},
eprint = {arXiv:1502.03240v1},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/conditional random fields as recurrent networks.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv.org},
pages = {2015},
pmid = {10463930},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
temp_url = {http://arxiv.org/abs/1502.03240v1{\%}5Cnfile:///Files/6C/6CB8A464-5750-4AAE-8859-75978D231F5D.pdf},
volume = {cs.CV},
year = {2015}
}
@article{Rastgoo,
author = {Rastgoo, Mojdeh and Massich, Joan and Freixenet, Jordi and Meyer-baese, Anke and Vilanova, Joan C and Meriaudeau, Fabrice},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/automatic prostate cancer detection through dce-mri.pdf:pdf},
title = {{Automatic prostate cancer detection through DCE-MRI images: all you need is a good normalization}}
}

@article{Vezhnevets2011,
author = {Vezhnevets, A and Ferrari, V and Buhmann, J},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/weakly supervised semantic segmentation with a multi image model.pdf:pdf},
isbn = {9781457711022},
journal = {Proc. Int'l Conf. Comput. Vis.},
title = {{Weakly Supervised Semantic Segmentation with Multi Image Model}},
year = {2011}
}

@article{Ghose2012,
author = {Ghose, Soumya and Oliver, Arnau and Mart{\'{i}}, Robert and Llad{\'{o}}, Xavier and Vilanova, Joan C and Freixenet, Jordi and Mitra, Jhimli and Sidib{\'{e}}, D{\'{e}}sir{\'{e}} and Meriaudeau, Fabrice},
doi = {10.1016/j.cmpb.2012.04.006},
file = {:home/charley/Documents/Datasets/prostate-master/Active learning for time series/prostrate{\_}segmentation.pdf:pdf},
issn = {0169-2607},
journal = {Comput. Methods Programs Biomed.},
keywords = {prostate gland segmentation},
number = {1},
pages = {262--287},
publisher = {Elsevier Ireland Ltd},
title = {{A survey of prostate segmentation methodologies in ultrasound , magnetic resonance and computed tomography images}},
temp_url = {http://dx.doi.org/10.1016/j.cmpb.2012.04.006},
volume = {108},
year = {2012}
}
@article{Wiener2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.1504v1},
author = {Wiener, Yair and Hanneke, Steve and El-Yaniv, R},
eprint = {arXiv:1404.1504v1},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiener, Hanneke, El-Yaniv - 2015 - A Compression Technique for Analyzing Disagreement-Based Active Learning.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {active learning,learning,pac,sample complexity,selective sampling,sequential design,statistical learning theory},
pages = {713--745},
title = {{A Compression Technique for Analyzing Disagreement-Based Active Learning}},
temp_url = {http://arxiv.org/abs/1404.1504},
volume = {16},
year = {2015}
}

@article{Settles2011,
abstract = {This article surveys recent work in active learning aimed at making it more practical for real-world use. In general, active learning systems aim to make machine learning more economical, since they can participate in the acquisition of their own training data. An active learner might iteratively select informative query instances to be labeled by an oracle, for example. Work over the last two decades has shown that such approaches are e ective at maintaining accuracy while reducing training set size in many machine learning applications. However, as we begin to deploy active learning in real ongoing learning systems and data annotation projects, we are encountering unexpected problems|due in part to practical realities that violate the basic assumptions of earlier foundational work. I review some of these issues, and discuss recent work being done to address the challenges.},
author = {Settles, Burr},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Settles - 2011 - From Theories to Queries Active Learning in Practice.pdf:pdf},
isbn = {9780971977761},
journal = {Proc. Work. Act. Learn. Exp. Des.},
keywords = {active learning,applied machine learning,human-computer interaction},
pages = {1--18},
title = {From Theories to Queries: Active Learning in Practice},
volume = {16},
year = {2011}
}

@article{Dasgupta2009,
abstract = {Active learning is defined by contrast to the passive model of supervised learning where all the labels for learning are obtained without reference to the learning algorithm, while in active learning the learner interactively chooses which data points to label. The hope of active learning is that interaction can substantially reduce the number of labels required, making solving problems via machine learning more practical. This hope is known to be valid in certain special cases, both empirically and theoretically. Variants of active learning have been investigated over several decades and fields. The focus of this tutorial is on general techniques which are applicable to many problems. At a mathematical level, this corresponds to approaches with provable guarantees under weakest-possible assumptions since real problems are more likely to fit algorithms which work under weak assumptions. We believe this tutorial should be of broad interest. People working on or using supervised learning are often confronted with the need for more labels, where active learning can help. Similarly, in reinforcement learning, generalizing while interacting in more complex ways is an active research topic. Please join us.},
author = {Dasgupta, Sanjoy and Langford, John},
doi = {10.1145/775047.775090},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta, Langford - 2009 - Active Learning Tutorial.pdf:pdf},
isbn = {158113567X},
issn = {{\textless}null{\textgreater}},
journal = {Int. Conf. Mach. Learn.},
title = {{Active Learning Tutorial}},
temp_url = {http://hunch.net/{~}active{\_}learning/},
year = {2009}
}

@article{Bodo2011,
author = {Bod{\'{o}}, Zal{\'{a}}n and Minier, Zsolt and Csat{\'{o}}, Lehel},
file = {:home/charley/Documents/Datasets/Active Image segmentation Notes/bodo11a.pdf:pdf},
journal = {Act. Learn. Exp. Des. @ AISTATS},
keywords = {active learning,large scale spectral clustering,normalized cuts,support vector},
pages = {127--139},
title = {{Active Learning with Clustering}},
temp_url = {http://msn.mtome.com/Publications/CiML/CiML-v6-book.pdf{\#}page=155},
volume = {16},
year = {2011}
}

@article{Planck2006,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. Nevertheless, on the first glance spectral clustering looks a bit mysterious, and it is not obvious to see why it works at all and what it really does. This article is a tutorial introduction to spectral clustering. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Planck, Max and Luxburg, Ulrike Von},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Planck, Luxburg - 2006 - A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering.pdf:pdf},
isbn = {0960-3174},
issn = {09603174},
journal = {Stat. Comput.},
keywords = {graph laplacian,spectral clustering},
number = {March},
pages = {395--416},
pmid = {19784854},
title = {A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering},
temp_url = {http://www.springerlink.com/index/10.1007/s11222-007-9033-z},
volume = {17},
year = {2006}
}

@article{Tong2001,
abstract = {Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option ofusing pool-based active learning. Instead ofusing a randomly selected training set, the learner has access to a pool ofunlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings},
author = {Tong, Simon and Koller, Daphne},
doi = {10.1162/153244302760185243},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tong, Koller - 2001 - Support Vector Machine Active Learning with Applications to Text Classification.pdf:pdf},
isbn = {1558607072},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {active learning,classifica-,relevance feedback,selective sampling,support vector machines,tion},
pages = {45--66},
title = {Support Vector Machine Active Learning with Applications to Text Classification},
year = {2001}
}

@article{Settles2010,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
file = {:home/charley/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Settles - 2010 - Active Learning Literature Survey(2).pdf:pdf},
issn = {00483931},
journal = {Mach. Learn.},
number = {2},
pages = {201--221},
title = {{Active Learning Literature Survey}},
volume = {15},
year = {2010}
}

@book{Sonka2007,
 author = {Sonka, Milan and Hlavac, Vaclav and Boyle, Roger},
 title = {Image Processing, Analysis, and Machine Vision},
 year = {2007},
 isbn = {049508252X},
 publisher = {Thomson-Engineering},
}

@book{Bishop1995,
 author = {Bishop, Christopher M.},
 title = {Neural Networks for Pattern Recognition},
 year = {1995},
 isbn = {0198538642},
 publisher = {Oxford University Press, Inc.},
 address = {New York, NY, USA},
}

@book{MacKay2002,
 author = {MacKay, David J. C.},
 title = {Information Theory, Inference \& Learning Algorithms},
 year = {2002},
 isbn = {0521642981},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}

@inproceedings{GouldFultonKoller2009,
  title={Decomposing a scene into geometric and semantically consistent regions},
  author={Gould, Stephen and Fulton, Richard and Koller, Daphne},
  booktitle={Computer Vision, 2009 IEEE 12th International Conference on},
  pages={1--8},
  year={2009},
  organization={IEEE}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    temp_url = {http://www.deeplearningbook.org},
    year={2016}
}

@misc{tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
temp_url={https://www.tensorflow.org/},
temp_note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@INPROCEEDINGS{SCH:KPCA:1999,
    author = {Bernhard Sch{\"o}lkopf and Alexander Smola and Klaus-Robert M{\"u}ller},
    title = {Kernel principal component analysis},
    editor = {Bernhard Sch{\"o}lkopf and Christopher J.C. Burges and Alexander Smola},
    booktitle = {Advances in Kernel Methods - Support Vector Learning},
    year = {1999},
    pages = {327--352},
    publisher = {MIT Press}
}

@book{SchSmo2001,
 author = {Scholkopf, Bernhard and Smola, Alexander J.},
 title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
 year = {2001},
 isbn = {0262194759},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
